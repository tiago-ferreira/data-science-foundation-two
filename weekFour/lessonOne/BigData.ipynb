{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "Uma boa definição para BigData seria, dados muitos numeros que nao podem ser processados por uma unica maquina.\n",
    "\n",
    "## Desafios do Big Data\n",
    "\n",
    "- Os dados sao criados muito rapidos\n",
    "- Vem de diferentes lugares com diferentes formatos\n",
    "\n",
    "## Os 3 V\n",
    "\n",
    "- Volume -> refere-se ao tamanho dos dados com quais esta se lidando\n",
    "- Variedade -> refere-se ao fato de os dados vierem de diferentes lugares e diferentes formatos\n",
    "- Velocidade -> refere-se a velocidade com que os dados são gerados\n",
    "\n",
    "## HDFS\n",
    "\n",
    "- HDFS replica todos os dados por segurança\n",
    "- Para armazenar 100TB de dado em um cluster do Hadoop você precisa de 300Tb de espaço de disco bruto por padrão\n",
    "\n",
    "### Se um dos nós que executa o daemon DataNode no cluster falha?\n",
    "\n",
    "Hadoop ira replicar automaticamente qualquer bloco que esteja naquela no\n",
    "\n",
    "\n",
    "\n",
    "### Quais precauções você pode tomar para reduzir a probabilidade dos problemas relacionados a falha do NameNode ocorrerem?\n",
    "\n",
    "- Configurar o NameNode para armazenar seu metadado em uma outra localização utilizando NFS\n",
    "- Configurar um NameNode em standby\n",
    "- Se assegurar que o NameNode está rodando em um hardware high-end\n",
    "\n",
    "\n",
    "### Ao executar um job de map reduce se o diretorio de saida ja existir o job se recusa a rodar\n",
    "\n",
    "\n",
    "### Porque o tamanho do bloco do Hadoop é configurado com 64MB por padrão quando a maior partes dos arquivos de sistema tem bloco de tamanho 16kb ou menos?\n",
    "\n",
    "Se o tamanho do bloco do Hadoop for configurado com 16kb, existiria um número grande de blocos em um mesmo cluster, o que faria que o NameNode tivesse que gerenciar um número muito alto de metadado\n",
    "\n",
    "Como precisamos de um Mapper para cada bloco que queremos processar, existiriam muitos mapeadores, cada um processando um pedaço do dados, o que não é eficiente\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
